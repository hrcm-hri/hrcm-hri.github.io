---
layout: layout_2025
urltitle:  "HRCM 2025: Human-Robot Contact and Manipulation"
title: "HRCM 2025: Human-Robot Contact and Manipulation"
categories: hri, contact, manipulation, compliant control, shared control, shared autonomy
permalink: /2025/
favicon: /2025/img/icon.png
bibtex: true
paper: true
acknowledgements: ""
# Ethan K. Gordon
---

<br>
<div class="row">
  <div class="col-xs-12">
    <img class="img-fluid" src="{{ "img/banner.png" | prepend:site.baseurl }}">
    <!--
    <small style="float:right;margin-top:0mm;margin-right:12mm;">Image credit to <a href="https://openai.com/dall-e-3" target="_blank">DALL·E</a></small>
    <br><br>
    <center>
      <table class="event-details">
        <tr><td class="item">Date:     </td><td class="desc">Tuesday, 18th June 2024      </td></tr>
        <tr><td class="item">Time:     </td><td class="desc">8:30 AM – 12:30 PM (half-day)</td></tr>
        <tr><td class="item">Location: </td><td class="desc">Arch 309                     </td></tr>
      </table>
    </center>
  -->
  </div>
</div><br>

<div class="row">
  <div class="col-xs-12"><a class="anchor" id="intro"></a>
    <h2>Introduction</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
    The Workshop on Human-Robot Contact and Manipulation (HRCM 2025) at aims to to unite expertise across control theory, shared autonomy, physical modeling, and human understanding to have robots operate in direct or indirect physical contact with people. The workshop topics include (but are not limited to):
    </p>
    <ul>
      <li>Compliant robot hardware design for human contact</li>
      <li>Applications involving direct human-robot contact, such as:
        <ul>
          <li>Assisted Activities for Daily Living (ADLs), e.g., feeding, bathing, walking</li>
          <li>Exoskeletons and active prostheses</li>
          <li>Medicine, e.g., patient transfer and surgery</li>
        </ul>
      </li>
      <li>Collaborative manipulation and shared control</li>
      <li>Kinesthetic teaching/shared autonomy/control of robots</li>
      <li>Tactile teleoperation and haptic feedback interfaces</li>
      <li>Compliant control for human physical safety and comfort</li>
      <li>Contact modeling and simulation for soft bodies</li>
      <li>Standardized benchmarks for HRCM</li>
    </ul>
  </div>
</div> <br>

<!--
<div class="row">
  <div class="col-xs-12 panel-group"><a class="anchor" id="calls"></a>
    <h2>Call for Contributions</h2>
    <br>
    <div class="panel panel-default">
      <div class="panel-heading" data-toggle="collapse" data-parent="#call" href="#call-papers" style="cursor:pointer;">
        <h3 style="margin:0;">Full Workshop Papers</h3>
      </div>
      <div id="call-papers" class="panel-collapse collapse" data-parent="#call">
        <div class="panel-body">
          <p>
	    <span style="font-weight:500;">Submission:</span> We invite authors to submit unpublished papers (8-page <a href="https://github.com/cvpr-org/author-kit/releases" target="_blank">CVPR format</a>) to our workshop, to be presented at a poster session upon acceptance. All submissions will go through a double-blind review process.
	    All contributions must be submitted (along with supplementary materials, if any) at this <a href="https://cmt3.research.microsoft.com/GAZE2024/Submission/Index">CMT link</a>.
	  </p>
	  <p>
	    Accepted papers will be published in the official CVPR Workshops proceedings and the Computer Vision Foundation (CVF) Open Access archive.
	  </p>
	  <p>
	    <span style="font-weight:500;">Note:</span> Authors of previously rejected main conference submissions are also welcome to submit their work to our workshop. When doing so, you must submit the previous reviewers' comments (named as <code>previous_reviews.pdf</code>) and a letter of changes (named as <code>letter_of_changes.pdf</code>) as part of your supplementary materials to clearly demonstrate the changes made to address the comments made by previous reviewers.
          </p>
        </div>
      </div>
    </div>
    <br>
  </div>
</div><br>

<div class="row">
  <div class="col-xs-12"><a class="anchor" id="dates"></a>
    <h2>Important Dates</h2>
    <br>
    <table class="table table-striped">
      <tbody>
        <tr>
          <td>Paper Submission Deadline</td>
          <td>March 15, 2024 (23:59 Pacific time)</td>
	  <td><span class="countdown" reference="15 Mar 2024 23:59:59 PST"></span></td>
        </tr>
        <tr>
          <td>Notification to Authors</td>
          <td>April 5, 2024</td>
        </tr>
        <tr>
          <td>Camera-Ready Deadline</td>
          <td>April 14, 2024</td>
        </tr>
        <tr>
          <td>Workshop Day</td>
          <td>June 18, 2024</td>
        </tr>
      </tbody>
    </table>
  </div>
</div><br>

<div class="row">
  <div class="col-xs-12"><a class="anchor" id="schedule"></a>
     <h2>Workshop Video Recording</h2>
     <br>
     <center>
       <iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/Vgrgi53efkE?si=jqeGBmH7deE41vq4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
     </center>
     <br>
  </div>
</div><br>

<div class="row">
  <div class="col-xs-12"><a class="anchor" id="schedule"></a>
     <h2>Workshop Schedule</h2>
     <br>
     <table class="table schedule" style="border:none !important;">
      <thead class="thead-light">
        <tr>
	  <th>Time in UTC</th>
	  <th>Start Time in UTC<span class="tz-offset"></span><b>*</b><br><span class="tz-subtext">(probably your time zone)</span></th>
          <th>Item</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>3:30pm - 3:35pm</td>
          <td class="to-local-time">18 Jun 2024 15:30:00 UTC</td>
          <td>Opening remark</td>
        </tr>
        <tr>
          <td>3:35pm - 4:15pm</td>
          <td class="to-local-time">18 Jun 2024 15:35:00 UTC</td>
          <td>Invited talk by Feng Xu</td>
        </tr>
        <tr>
          <td>4:15pm - 4:55pm</td>
          <td class="to-local-time">18 Jun 2024 16:15:00 UTC</td>
          <td>Invited talk by Alexander Fix</td>
        </tr>
        <tr>
          <td>4:55pm - 5:10pm</td>
          <td class="to-local-time">18 Jun 2024 16:55:00 UTC</td>
          <td>Invited poster spotlight talk</td>
        </tr>
        <tr>
          <td>5:10pm - 6:10pm</td>
          <td class="to-local-time">18 Jun 2024 17:10:00 UTC</td>
          <td>Coffee break & poster presentation</td>
        </tr>
        <tr>
          <td>6:10pm - 6:50pm</td>
          <td class="to-local-time">18 Jun 2024 18:10:00 UTC</td>
          <td>Workshop paper presentation</td>
        </tr>
         <tr>
          <td>6:50pm - 7:00pm</td>
          <td class="to-local-time">18 Jun 2022 18:50:00 UTC</td>
          <td>Panel discussion</td>
        </tr>
        <tr>
          <td>6:50pm - 7:00pm</td>
          <td class="to-local-time">18 Jun 2024 18:50:00 UTC</td>
          <td>Award & closing remark</td>
        </tr>
        <tr>
          <td>8:15pm - 8:20pm</td>
          <td class="to-local-time">20 Jun 2021 20:15:00 UTC</td>
          <td>Award & closing remark</td>
        </tr>
      </tbody>
     </table>
     <span class="disclaimer">
     * This time is calculated to be in your computer's reported time zone.
     <br>
     For example, those in Los Angeles may see UTC-7,
     <br>
     while those in Berlin may see UTC+2.
     <br>
     <br>
     Please note that there may be differences to your actual time zone.</span>
  </div>
</div><br>

<div class="row">
  <div class="col-xs-12"><a class="anchor" id="speakers"></a>
    <h2>Invited Keynote Speakers</h2>
    <br>

    <div class="row speaker">
      <div class="col-sm-3 speaker-pic">
        <a href="http://xufeng.site/">
          <img class="people-pic" src="/2024/img/people/fx.jpg" />
        </a>
        <div class="people-name">
          <a href="http://xufeng.site/">Feng Xu</a>
          <h6>Tsinghua University</h6>
        </div>
      </div>
      <div class="col-sm-9">
        <h3>Eye Region Reconstruction with a Monocular Camera</h3><br />
        <b>Abstract</b>
        <p class="speaker-abstract">Eye region reconstruction is an important yet challenging task in computer vision and graphics. It suffers from complicated geometry and motions, severe occlusions, and eyeglass interference, for which existing methods have to make a trade-off between capture cost and reconstruction quality. We focused on low-cost capture setups and proposed novel algorithms to achieve high-quality eye region reconstruction under limited inputs. In addition, we tried to solve the eyeglass interference, which lays the foundation for high-quality eye region reconstruction. We have also tried to apply eye region reconstruction in medicine for disease diagnosis.</p>

        <div class="panel panel-default">
          <div class="panel-heading" data-toggle="collapse" href="#jr-bio" style="cursor:pointer;text-align:center">
            <b>Biography <span style="font-weight:normal">(click to expand/collapse)</span></b>
          </div>
          <div id="jr-bio" class="panel-collapse">
	    <div class="panel-body">
              <p class="speaker-bio">
	        Feng Xu is currently an associate professor at the School of Software, Tsinghua University, Beijing, China. He earned a Ph.D. in automation and a B.S. in physics from Tsinghua University in 2012 and 2007, respectively. Until 2015, He was a Researcher in the Internet Graphics group, Microsoft Research Asia. His research interests include human body reconstruction, face animation, and medical image analysis. He has authored more than 40 conference and journal papers in the corresponding areas, including Nature Medicine, SIGGRAPH, CVPR, ICCV, ECCV, PAMI, and so on.
              </p>
	    </div>
          </div>
        </div>
      </div>
    </div>

    <div class="row speaker">
      <div class="col-sm-3 speaker-pic">
        <a href="https://about.meta.com/realitylabs/">
	  <img class="people-pic" src="/2024/img/people/af.jpg" />
	</a>
        <div class="people-name">
          <a href="https://about.meta.com/realitylabs/">Alexander Fix</a>
          <h6>Meta Reality Labs Research</h6>
        </div>
      </div>
      <div class="col-sm-9">
        <h3>Challenges in Near Eye Tracking for AR/VR</h3><br />
        <b>Abstract</b>
        <p class="speaker-abstract">Artificial and Virtual Reality (AR/VR) has incredible potential for using eye tracking to power the future of computing, but also incredible challenges in making eye tracking that works for everyone, all the time. In this talk, I will talk about some of the work we’re doing here at Meta Reality Labs to build eye tracking into AR/VR, as well as the key areas where the CVPR and GAZE 2024 community can help solve the hardest problems in this space. We will highlight Aria – the ET-enabled research glasses from Meta – and how they are a great tool for investigating applications of eye tracking. We will also show some new approaches to doing eye tracking, based on event cameras, polarization, and more.</p>

        <div class="panel panel-default">
          <div class="panel-heading" data-toggle="collapse" href="#jr-bio" style="cursor:pointer;text-align:center">
            <b>Biography <span style="font-weight:normal">(click to expand/collapse)</span></b>
          </div>
          <div id="jr-bio" class="panel-collapse">
	    <div class="panel-body">
              <p class="speaker-bio">
	        Alexander Fix is a Research Scientist at Meta Reality Labs Research, where he has worked on eye tracking and related topics for the last 9 years. His research interests include 3D reconstruction, NeRF and other implicit reconstructions, and geometric eye tracking. Collaborations at Meta include quite a lot of eye tracking hardware research, particularly on novel methods for eye tracking such as Event Cameras. He graduated from Cornell in 2016 with a PhD in Computer Science, and from the University of Chicago in 2009 with a BS in Computer Science and Mathematics.
              </p>
	    </div>
          </div>
        </div>
      </div>
    </div>

  </div>
</div>

<div class="row">
  <div class="col-xs-12"><a class="anchor" id="accepted-papers"></a>
    <h2>Accepted Full Papers</h2>

    <div class="paper">
      <span class="title">Spatio-Temporal Attention and Gaussian Processes for Personalized Video Gaze Estimation
</span>
      <span class="authors">Swati Jindal, Mohit Yadav, Roberto Manduchi</span>
      <span class="award">Best Paper Award</span>
      <div class="btn-group btn-group-xs" role="group">
        <button class="btn btn-success">GAZE 2024</button>
	<button class="btn btn-poster-id">Poster #8</button>
	<a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2024W/GAZE/papers/Jindal_Spatio-Temporal_Attention_and_Gaussian_Processes_for_Personalized_Video_Gaze_Estimation_CVPRW_2024_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
	<a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2024W/GAZE/supplemental/Jindal_Spatio-Temporal_Attention_and_CVPRW_2024_supplemental.pdf"><i class="fas fa-file-pdf"></i> Suppl. (CVF)</a>
	<a class="btn btn-default" target="_blank" href="https://arxiv.org/abs/2404.05215"><i class="fas fa-archive"></i> arXiv</a>
        <a class="btn btn-default" target="_blank" href="VIDEO URL"><i class="fas fa-video"></i> Video</a>
	 <a class="btn btn-default" target="_blank" href="https://github.com/jswati31/stage"><i class="fas fa-code"></i> Code</a>
      </div>
    </div>

    <div class="paper">
      <span class="title">Exploring the Zero-Shot Capabilities of Vision-Language Models for Improving Gaze Following
</span>
      <span class="authors">Anshul Gupta, Pierre Vuillecard, Arya Farkhondeh, Jean-Marc Odobez</span>
      <span class="award">Best Paper Award</span>
      <div class="btn-group btn-group-xs" role="group">
        <button class="btn btn-success">GAZE 2024</button>
	<button class="btn btn-poster-id">Poster #9</button>
	<a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2024W/GAZE/papers/Gupta_Exploring_the_Zero-Shot_Capabilities_of_Vision-Language_Models_for_Improving_Gaze_CVPRW_2024_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
	<a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2024W/GAZE/supplemental/Gupta_Exploring_the_Zero-Shot_CVPRW_2024_supplemental.pdf"><i class="fas fa-file-pdf"></i> Suppl. (CVF)</a>
        <a class="btn btn-default" target="_blank" href="ARXIV URL"><i class="fas fa-archive"></i> arXiv</a>
        <a class="btn btn-default" target="_blank" href="VIDEO URL"><i class="fas fa-video"></i> Video</a>
      </div>
    </div>

    <div class="paper">
      <span class="title">Gaze Scanpath Transformer: Predicting Visual Search Target by Spatiotemporal Semantic Modeling of Gaze Scanpath</span>
      <span class="authors">Takumi Nishiyasu, Yoichi Sato</span>
      <span class="award">Best Paper Award</span>
      <div class="btn-group btn-group-xs" role="group">
        <button class="btn btn-success">GAZE 2024</button>
	<button class="btn btn-poster-id">Poster #10</button>
	<a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2024W/GAZE/papers/Nishiyasu_Gaze_Scanpath_Transformer_Predicting_Visual_Search_Target_by_Spatiotemporal_Semantic_CVPRW_2024_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
        <a class="btn btn-default" target="_blank" href="CVF SUPPL URL"><i class="fas fa-file-pdf"></i> Suppl. (CVF)</a>
        <a class="btn btn-default" target="_blank" href="ARXIV URL"><i class="fas fa-archive"></i> arXiv</a>
        <a class="btn btn-default" target="_blank" href="VIDEO URL"><i class="fas fa-video"></i> Video</a>
      </div>
    </div>

    <div class="paper">
      <span class="title">GESCAM: A Dataset and Method on Gaze Estimation for Classroom Attention Measurement</span>
      <span class="authors">Athul Mathew, Arshad Khan, Thariq Khalid, Riad Souissi</span>
      <span class="award">Best Paper Award</span>
      <div class="btn-group btn-group-xs" role="group">
        <button class="btn btn-success">GAZE 2024</button>
	<button class="btn btn-poster-id">Poster #11</button>
	<a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2024W/GAZE/papers/Mathew_GESCAM__A_Dataset_and_Method_on_Gaze_Estimation_for_CVPRW_2024_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
        <a class="btn btn-default" target="_blank" href="CVF SUPPL URL"><i class="fas fa-file-pdf"></i> Suppl. (CVF)</a>
	<a class="btn btn-default" target="_blank" href="https://athulmmathew.github.io/GESCAM/"><i class="fas fa-globe"></i> Project Page</a>
        <a class="btn btn-default" target="_blank" href="ARXIV URL"><i class="fas fa-archive"></i> arXiv</a>
        <a class="btn btn-default" target="_blank" href="VIDEO URL"><i class="fas fa-video"></i> Video</a>
      </div>
    </div>

  </div>
</div><br>

<div class="row">
  <div class="col-xs-12"><a class="anchor" id="invited-posters"></a>
    <h2>Invited Posters</h2>

    <div class="paper">
      <span class="title">What Do You See in Vehicle? Comprehensive Vision Solution for In-Vehicle Gaze Estimation</span>
      <span class="authors">Yihua Cheng, Yaning Zhu, Zongji Wang, Hongquan Hao, Liu Wei, Shiqing Cheng, Xi Wang, Hyung Jin Chang</span>
      <div class="btn-group btn-group-xs" role="group">
        <button class="btn btn-primary">CVPR 2024</button>
	<button class="btn btn-poster-id">Poster #12</button>
	<a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2024/papers/Cheng_What_Do_You_See_in_Vehicle_Comprehensive_Vision_Solution_for_CVPR_2024_paper.pdf"><i class="fas fa-file-pdf"></i> PDF (CVF)</a>
	<a class="btn btn-default" target="_blank" href="https://openaccess.thecvf.com/content/CVPR2024/supplemental/Cheng_What_Do_You_CVPR_2024_supplemental.pdf"><i class="fas fa-file-pdf"></i> Suppl. (CVF)</a>
	<a class="btn btn-default" target="_blank" href="https://yihua.zone/work/ivgaze/"><i class="fas fa-globe"></i> Project Page</a>
	<a class="btn btn-default" target="_blank" href="https://arxiv.org/abs/2403.15664"><i class="fas fa-archive"></i> arXiv</a>
	<a class="btn btn-default" target="_blank" href="https://github.com/yihuacheng/IVGaze"><i class="fas fa-code"></i> Code</a>
        <a class="btn btn-default" target="_blank" href="SUPPL PDF"><i class="fas fa-file-pdf"></i> Supp.</a>
        <a class="btn btn-default" target="_blank" href="VIDEO URL"><i class="fas fa-video"></i> Video</a>
      </div>
    </div>
  </div>
</div><br>
-->

<div class="row">
  <div class="col-xs-12"><a class="anchor" id="organizers"></a>
    <h2>Organizers</h2>
  </div>
</div>

<br><br>

<div class="row">
  <div class="col-xs-1"></div>
  <div class="col-xs-2">
    <a href="https://ethankgordon.com/">
      <center><img class="people-pic" src="{{ "img/people/ekg.png" | prepend:site.baseurl }}"></center>
    </a>
    <div class="people-name">
      <a href="https://ethankgordon.com/">Ethan K. Gordon</a>
      <h6>University of Pennsylvania</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="http://imtianyuli.com/">
      <center><img class="people-pic" src="{{ "img/people/tl.jpg" | prepend:site.baseurl }}"></center>
    </a>
    <div class="people-name">
      <a href="http://imtianyuli.com/">Tianyu Li</a>
      <h6>University of Pennsylvania</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://nbfigueroa.github.io/">
      <center><img class="people-pic" src="{{ "img/people/nf.jpg" | prepend:site.baseurl }}"></center>
    </a>
    <div class="people-name">
      <a href="https://nbfigueroa.github.io/">Nadia Figueroa</a>
      <h6>University of Pennsylvania</h6>
    </div>
  </div>
  <div class="col-xs-2">
    <a href="https://emprise.cs.cornell.edu/">
      <center><img class="people-pic" src="{{ "img/people/tb.jpg" | prepend:site.baseurl }}"></center>
    </a>
    <div class="people-name">
      <a href="https://emprise.cs.cornell.edu/">Tapomayukh Bhattacharjee</a>
      <h6>Cornell University</h6>
    </div>
  </div>
</div><br>

<div class="row">
  <div class="col-xs-12"><a class="anchor" id="organizers"></a>
    <h2>Website Chair</h2>
  </div>
</div>

<br>

<div class="row">
  <div class="col-xs-1"></div>
  <div class="col-xs-2">
    <a href="https://ethankgordon.com/">
      <img class="people-pic" src="{{ "img/people/ekg.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://ethankgordon.com/">Ethan K. Gordon</a>
      <h6>University of Pennsylvania</h6>
    </div>
  </div>
  <br><br>
  <div class="col-xs-8">
    Please contact me if you have any question about this website.
    <br>
    Email: <span id="email">ethankg@sea<b>harvester obfuscation</b>s.upenn.edu</span>
    <!-- See https://spencermortensen.com/articles/email-obfuscation/#text-display -->
  </div>
  <div class="col-xs-1"></div>
</div>
<br>


<!--
<div class="row">
  <div class="col-xs-12"><a class="anchor" id="sponsors"></a>
    <h2>Workshop sponsored by:</h2>
  </div>
</div>

<div class="row">
  <div class="col-xs-4 sponsor">
    <a href="https://www.nvidia.com/"><img src="img/nvidia.jpg" /></a>
  </div>
  <div class="col-xs-4 sponsor">
    <a href="https://www.birmingham.ac.uk/"><img src="img/uob.jpg" /></a>
  </div>
  <div class="col-xs-4 sponsor">
    <a href="https://www.google.com/"><img src="img/google.png" /></a>
  </div>
</div>
-->
